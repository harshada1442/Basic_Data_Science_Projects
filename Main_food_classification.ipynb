{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshada1442/Basic_Data_Science_Projects/blob/main/Main_food_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iANkV9v_-pc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4qoyhjZAFCb",
        "outputId": "cbfa4329-a569-4370-ae5c-b57e4f1a4263"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfzNzSENCT9J"
      },
      "source": [
        "#                Data Processing\n",
        "\n",
        "> We will resize all images in the input folder to the specified dimensions (in this case, 224x224 pixels) and save them in the output folder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS3V7-F9APwi"
      },
      "outputs": [],
      "source": [
        "def resize_images(input_folder, output_folder, target_width=224, target_height=224):\n",
        "    # Iterate through all subdirectories (assuming each subdirectory is a category of food)\n",
        "    for category in os.listdir(input_folder):\n",
        "        category_input_folder = os.path.join(input_folder, category)\n",
        "        category_output_folder = os.path.join(output_folder, category)\n",
        "        os.makedirs(category_output_folder, exist_ok=True)\n",
        "\n",
        "        # Iterate through all files in the current category folder\n",
        "        for filename in os.listdir(category_input_folder):\n",
        "            if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
        "                # Read the image\n",
        "                img_path = os.path.join(category_input_folder, filename)\n",
        "                img = cv2.imread(img_path)\n",
        "\n",
        "                # Resize the image to the target dimensions\n",
        "                resized_img = cv2.resize(img, (target_width, target_height))\n",
        "\n",
        "                # Define the output path for the resized image dynamically\n",
        "                output_path = os.path.join(category_output_folder, filename)\n",
        "\n",
        "                # Save the resized image\n",
        "                cv2.imwrite(output_path, resized_img)\n",
        "                print(f'Resized and saved: {output_path}')\n",
        "\n",
        "# Define the paths to the folder containing your food images and the output folder\n",
        "input_folder = '/content/drive/MyDrive/Food Recognization/Images'\n",
        "output_folder = '/content/drive/MyDrive/Food Recognization/resized_images/'\n",
        "\n",
        "# Call the function to resize images in the input folder and save them to the output folder\n",
        "resize_images(input_folder, output_folder)\n",
        "\n",
        "print('Resizing completed.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzO79xj2HiTS"
      },
      "source": [
        "**Normalize the data**\n",
        "we will Normalize the pixel values of the all images to a common scale, usually between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vout1Op2CGga"
      },
      "outputs": [],
      "source": [
        "def normalize_images(input_folder, output_folder, target_width=224, target_height=224):\n",
        "    # Iterate through all subfolders (categories) in the input folder\n",
        "    for category in os.listdir(input_folder):\n",
        "        category_input_folder = os.path.join(input_folder, category)\n",
        "        category_output_folder = os.path.join(output_folder, category)\n",
        "        os.makedirs(category_output_folder, exist_ok=True)\n",
        "\n",
        "        # Iterate through all files in the current category folder\n",
        "        for filename in os.listdir(category_input_folder):\n",
        "            if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
        "                # Read the image\n",
        "                img_path = os.path.join(category_input_folder, filename)\n",
        "                img = cv2.imread(img_path)\n",
        "\n",
        "                # Resize the image to the target dimensions\n",
        "                resized_img = cv2.resize(img, (target_width, target_height))\n",
        "\n",
        "                # Normalize pixel values to the range [0, 1]\n",
        "                normalized_img = resized_img.astype(np.float32) / 255.0\n",
        "\n",
        "                # Define the output path for the normalized image\n",
        "                output_path = os.path.join(category_output_folder, filename)\n",
        "\n",
        "                # Save the normalized image\n",
        "                cv2.imwrite(output_path, (normalized_img * 255).astype(np.uint8))\n",
        "                print(f'Normalized and saved: {output_path}')\n",
        "\n",
        "input_main_folder = '/content/drive/MyDrive/Food Recognization/Images'\n",
        "output_main_folder = '/content/drive/MyDrive/Food Recognization/resized_images/'\n",
        "\n",
        "# Call the function to normalize images in the input folder and save them to the output folder\n",
        "normalize_images(input_main_folder, output_main_folder)\n",
        "\n",
        "print('Normalization completed.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBrmK3vpH6-t"
      },
      "source": [
        "# Split the dataset\n",
        "In this section we will Divide our dataset into three parts: training, validation, and testing sets. A common split is 70% for training, 15% for validation, and 15% for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrP6CDC_QQfU",
        "outputId": "7a21c0d6-e65e-453c-f019-64c9f18f9f33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset split completed.\n"
          ]
        }
      ],
      "source": [
        "# Define the path to your dataset directory on Google Drive\n",
        "dataset_dir = '/content/drive/MyDrive/Food Recognization/resized_images'\n",
        "\n",
        "# Define the paths for the training and testing datasets on Google Drive\n",
        "train_dir = '/content/drive/MyDrive/Food Recognization/Split_data/training_set/'\n",
        "test_dir = '/content/drive/MyDrive/Food Recognization/Split_data/testing_set/'\n",
        "\n",
        "# Create output directories if they don't exist\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Define the percentage split for training\n",
        "train_percent = 0.7  # 70% for training\n",
        "\n",
        "# Iterate through the subdirectories in the dataset directory (assuming each subdirectory represents a class)\n",
        "for class_name in os.listdir(dataset_dir):\n",
        "    class_dir = os.path.join(dataset_dir, class_name)\n",
        "\n",
        "    # List all images in the class directory\n",
        "    class_images = os.listdir(class_dir)\n",
        "\n",
        "    # Calculate the number of images for training based on the desired train_percent\n",
        "    num_train_images = int(len(class_images) * train_percent)\n",
        "\n",
        "    # Ensure at least one image is included in the training set\n",
        "    num_train_images = max(1, num_train_images)\n",
        "\n",
        "    # Select the first num_train_images for training and the rest for testing\n",
        "    train_images = class_images[:num_train_images]\n",
        "    test_images = class_images[num_train_images:]\n",
        "\n",
        "    # Move images to the training directory\n",
        "    for image in train_images:\n",
        "        src_path = os.path.join(class_dir, image)\n",
        "        dst_path = os.path.join(train_dir, class_name, image)\n",
        "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "        shutil.copy(src_path, dst_path)\n",
        "\n",
        "    # Move images to the testing directory\n",
        "    for image in test_images:\n",
        "        src_path = os.path.join(class_dir, image)\n",
        "        dst_path = os.path.join(test_dir, class_name, image)\n",
        "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "        shutil.copy(src_path, dst_path)\n",
        "\n",
        "print(\"Dataset split completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSrka3dSSVDp"
      },
      "source": [
        "# Deep Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QefFgj1nSBUC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNWXQj7LfebU",
        "outputId": "7856f70c-8e4b-406d-d07b-c625392c6d14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 421 images belonging to 22 classes.\n"
          ]
        }
      ],
      "source": [
        "train_data_generator = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "training_set = train_data_generator.flow_from_directory('/content/drive/MyDrive/Food Recognization/Split_data/training_set', target_size=(64, 64), batch_size=32, class_mode='categorical')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hPZHHw6gX8E",
        "outputId": "35f864a1-2048-4417-8613-bdc982f251f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "food_categories=[\"vada_pav\", \"uttapa\", \"upit\", \"sheera\", \"samosa\", \"sabudana_vada\", \"sabudana_khichdi\", \"poha\", \"pav_bhaji\", \"pani_puri\", \"Onion_pakoda\", \"misal_pav\", \"medu_vada\", \"masala_dosa\", \"kacchi_dabeli\", \"jalebi\", \"Idali_sambhar_chutany\", \"chole_bhature\",\"Bhel_puri\", \"batata_vada\",\"aloo_puri\", \"aloo_tikii\"]\n",
        "len(food_categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpswVVG6f_vd"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(64, 64, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(units=len(food_categories), activation='softmax'))  # Adjust len(food_categories) based on the number of food categories you have\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69FyI1ksgoOd",
        "outputId": "254550bf-1817-4377-e59d-4969e4b8cdf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "14/14 [==============================] - 137s 10s/step - loss: 3.1053 - accuracy: 0.0736\n",
            "Epoch 2/25\n",
            "14/14 [==============================] - 3s 212ms/step - loss: 3.0039 - accuracy: 0.1164\n",
            "Epoch 3/25\n",
            "14/14 [==============================] - 4s 240ms/step - loss: 2.9396 - accuracy: 0.1330\n",
            "Epoch 4/25\n",
            "14/14 [==============================] - 4s 305ms/step - loss: 2.8986 - accuracy: 0.1639\n",
            "Epoch 5/25\n",
            "14/14 [==============================] - 4s 272ms/step - loss: 2.8683 - accuracy: 0.1568\n",
            "Epoch 6/25\n",
            "14/14 [==============================] - 4s 295ms/step - loss: 2.8532 - accuracy: 0.1496\n",
            "Epoch 7/25\n",
            "14/14 [==============================] - 3s 219ms/step - loss: 2.8181 - accuracy: 0.1591\n",
            "Epoch 8/25\n",
            "14/14 [==============================] - 3s 223ms/step - loss: 2.7805 - accuracy: 0.1829\n",
            "Epoch 9/25\n",
            "14/14 [==============================] - 4s 233ms/step - loss: 2.6919 - accuracy: 0.1948\n",
            "Epoch 10/25\n",
            "14/14 [==============================] - 3s 212ms/step - loss: 2.7160 - accuracy: 0.1734\n",
            "Epoch 11/25\n",
            "14/14 [==============================] - 4s 267ms/step - loss: 2.6033 - accuracy: 0.1971\n",
            "Epoch 12/25\n",
            "14/14 [==============================] - 4s 231ms/step - loss: 2.5864 - accuracy: 0.2185\n",
            "Epoch 13/25\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 2.6294 - accuracy: 0.2138\n",
            "Epoch 14/25\n",
            "14/14 [==============================] - 4s 250ms/step - loss: 2.5819 - accuracy: 0.2280\n",
            "Epoch 15/25\n",
            "14/14 [==============================] - 5s 317ms/step - loss: 2.4997 - accuracy: 0.2352\n",
            "Epoch 16/25\n",
            "14/14 [==============================] - 3s 217ms/step - loss: 2.4043 - accuracy: 0.2779\n",
            "Epoch 17/25\n",
            "14/14 [==============================] - 3s 222ms/step - loss: 2.3987 - accuracy: 0.2732\n",
            "Epoch 18/25\n",
            "14/14 [==============================] - 4s 273ms/step - loss: 2.3677 - accuracy: 0.2732\n",
            "Epoch 19/25\n",
            "14/14 [==============================] - 3s 218ms/step - loss: 2.3613 - accuracy: 0.2755\n",
            "Epoch 20/25\n",
            "14/14 [==============================] - 3s 229ms/step - loss: 2.4145 - accuracy: 0.2898\n",
            "Epoch 21/25\n",
            "14/14 [==============================] - 5s 346ms/step - loss: 2.3242 - accuracy: 0.2850\n",
            "Epoch 22/25\n",
            "14/14 [==============================] - 3s 215ms/step - loss: 2.2492 - accuracy: 0.3207\n",
            "Epoch 23/25\n",
            "14/14 [==============================] - 3s 215ms/step - loss: 2.1294 - accuracy: 0.3587\n",
            "Epoch 24/25\n",
            "14/14 [==============================] - 5s 329ms/step - loss: 2.0980 - accuracy: 0.3610\n",
            "Epoch 25/25\n",
            "14/14 [==============================] - 4s 233ms/step - loss: 2.0416 - accuracy: 0.3634\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f6f390ca4a0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(training_set, epochs=25)  # You can adjust the number of epochs based on the performance on your validation set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXw9PtASN3Mg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyC2EXhyjuY6"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8olS7XwDNo2L"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Define the number of classes (food categories) in your dataset\n",
        "num_classes = 22  # Replace 10 with the actual number of food categories in your dataset\n",
        "\n",
        "# Define your CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))  # num_classes is the number of food categories\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJ3Ym1pwPiSQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lN_fmlhN4mA",
        "outputId": "80d8ca96-9ede-430e-931e-89bb86dc024f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "14/14 [==============================] - 81s 6s/step - loss: 3.1523 - accuracy: 0.0451\n",
            "Epoch 2/25\n",
            "14/14 [==============================] - 4s 271ms/step - loss: 3.0174 - accuracy: 0.1164\n",
            "Epoch 3/25\n",
            "14/14 [==============================] - 5s 342ms/step - loss: 2.9568 - accuracy: 0.1449\n",
            "Epoch 4/25\n",
            "14/14 [==============================] - 3s 232ms/step - loss: 2.9207 - accuracy: 0.1473\n",
            "Epoch 5/25\n",
            "14/14 [==============================] - 3s 226ms/step - loss: 2.8603 - accuracy: 0.1568\n",
            "Epoch 6/25\n",
            "14/14 [==============================] - 4s 308ms/step - loss: 2.7605 - accuracy: 0.1615\n",
            "Epoch 7/25\n",
            "14/14 [==============================] - 4s 288ms/step - loss: 2.6486 - accuracy: 0.1971\n",
            "Epoch 8/25\n",
            "14/14 [==============================] - 4s 249ms/step - loss: 2.5440 - accuracy: 0.2162\n",
            "Epoch 9/25\n",
            "14/14 [==============================] - 5s 326ms/step - loss: 2.4894 - accuracy: 0.2447\n",
            "Epoch 10/25\n",
            "14/14 [==============================] - 5s 300ms/step - loss: 2.3040 - accuracy: 0.3183\n",
            "Epoch 11/25\n",
            "14/14 [==============================] - 4s 251ms/step - loss: 2.1957 - accuracy: 0.3088\n",
            "Epoch 12/25\n",
            "14/14 [==============================] - 4s 253ms/step - loss: 2.1616 - accuracy: 0.3563\n",
            "Epoch 13/25\n",
            "14/14 [==============================] - 5s 303ms/step - loss: 2.0135 - accuracy: 0.4062\n",
            "Epoch 14/25\n",
            "14/14 [==============================] - 4s 247ms/step - loss: 1.8755 - accuracy: 0.4584\n",
            "Epoch 15/25\n",
            "14/14 [==============================] - 3s 241ms/step - loss: 1.7569 - accuracy: 0.4679\n",
            "Epoch 16/25\n",
            "14/14 [==============================] - 6s 450ms/step - loss: 1.6676 - accuracy: 0.4941\n",
            "Epoch 17/25\n",
            "14/14 [==============================] - 4s 295ms/step - loss: 1.5411 - accuracy: 0.5582\n",
            "Epoch 18/25\n",
            "14/14 [==============================] - 4s 252ms/step - loss: 1.5025 - accuracy: 0.5511\n",
            "Epoch 19/25\n",
            "14/14 [==============================] - 5s 367ms/step - loss: 1.3531 - accuracy: 0.5914\n",
            "Epoch 20/25\n",
            "14/14 [==============================] - 4s 248ms/step - loss: 1.2927 - accuracy: 0.6105\n",
            "Epoch 21/25\n",
            "14/14 [==============================] - 6s 401ms/step - loss: 1.2487 - accuracy: 0.6271\n",
            "Epoch 22/25\n",
            "14/14 [==============================] - 4s 263ms/step - loss: 1.1158 - accuracy: 0.6936\n",
            "Epoch 23/25\n",
            "14/14 [==============================] - 5s 379ms/step - loss: 1.0399 - accuracy: 0.7102\n",
            "Epoch 24/25\n",
            "14/14 [==============================] - 4s 242ms/step - loss: 0.9123 - accuracy: 0.7363\n",
            "Epoch 25/25\n",
            "14/14 [==============================] - 4s 259ms/step - loss: 0.9184 - accuracy: 0.7435\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7999dff8db10>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(training_set, epochs=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBVMUV27Pkaf",
        "outputId": "329ba103-c1fe-4dc6-e41f-28ba05be39d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 421 images belonging to 22 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "14/14 [==============================] - 60s 4s/step - loss: 3.7348 - accuracy: 0.0736\n",
            "Epoch 2/50\n",
            "14/14 [==============================] - 66s 5s/step - loss: 3.0678 - accuracy: 0.1093\n",
            "Epoch 3/50\n",
            "14/14 [==============================] - 54s 4s/step - loss: 3.0085 - accuracy: 0.1188\n",
            "Epoch 4/50\n",
            "14/14 [==============================] - 54s 4s/step - loss: 3.0266 - accuracy: 0.1188\n",
            "Epoch 5/50\n",
            "14/14 [==============================] - 53s 4s/step - loss: 2.9676 - accuracy: 0.1116\n",
            "Epoch 6/50\n",
            "14/14 [==============================] - 53s 4s/step - loss: 2.9445 - accuracy: 0.1164\n",
            "Epoch 7/50\n",
            "14/14 [==============================] - 53s 4s/step - loss: 2.9445 - accuracy: 0.1306\n",
            "Epoch 8/50\n",
            "14/14 [==============================] - 53s 4s/step - loss: 2.9581 - accuracy: 0.1211\n",
            "Epoch 9/50\n",
            "14/14 [==============================] - 52s 4s/step - loss: 2.8962 - accuracy: 0.1283\n",
            "Epoch 10/50\n",
            "14/14 [==============================] - 62s 4s/step - loss: 2.8593 - accuracy: 0.1496\n",
            "Epoch 11/50\n",
            "14/14 [==============================] - 54s 4s/step - loss: 2.8429 - accuracy: 0.1425\n",
            "Epoch 12/50\n",
            "14/14 [==============================] - 54s 4s/step - loss: 2.7863 - accuracy: 0.1686\n",
            "Epoch 13/50\n",
            "14/14 [==============================] - 56s 4s/step - loss: 2.8089 - accuracy: 0.1544\n",
            "Epoch 14/50\n",
            "14/14 [==============================] - 53s 4s/step - loss: 2.7838 - accuracy: 0.1710\n",
            "Epoch 15/50\n",
            "14/14 [==============================] - 53s 4s/step - loss: 2.7499 - accuracy: 0.1686\n",
            "Epoch 16/50\n",
            "14/14 [==============================] - 53s 4s/step - loss: 2.7390 - accuracy: 0.1948\n",
            "Epoch 17/50\n",
            "14/14 [==============================] - 55s 4s/step - loss: 2.6705 - accuracy: 0.1948\n",
            "Epoch 18/50\n",
            "14/14 [==============================] - 54s 4s/step - loss: 2.6807 - accuracy: 0.1876\n",
            "Epoch 19/50\n",
            "14/14 [==============================] - 55s 4s/step - loss: 2.6408 - accuracy: 0.2114\n",
            "Epoch 20/50\n",
            "14/14 [==============================] - 55s 4s/step - loss: 2.5510 - accuracy: 0.2067\n",
            "Epoch 21/50\n",
            "14/14 [==============================] - 56s 4s/step - loss: 2.6171 - accuracy: 0.1900\n",
            "Epoch 22/50\n",
            "14/14 [==============================] - 54s 4s/step - loss: 2.5750 - accuracy: 0.2114\n",
            "Epoch 23/50\n",
            "14/14 [==============================] - 54s 4s/step - loss: 2.6058 - accuracy: 0.2090\n",
            "Epoch 24/50\n",
            "14/14 [==============================] - 55s 4s/step - loss: 2.5533 - accuracy: 0.2067\n",
            "Epoch 25/50\n",
            "14/14 [==============================] - 56s 4s/step - loss: 2.6086 - accuracy: 0.2138\n",
            "Epoch 26/50\n",
            "14/14 [==============================] - 55s 4s/step - loss: 2.5027 - accuracy: 0.2185\n",
            "Epoch 27/50\n",
            "14/14 [==============================] - 56s 4s/step - loss: 2.4416 - accuracy: 0.2162\n",
            "Epoch 28/50\n",
            "14/14 [==============================] - 56s 4s/step - loss: 2.3762 - accuracy: 0.2708\n",
            "Epoch 29/50\n",
            "14/14 [==============================] - 58s 4s/step - loss: 2.4110 - accuracy: 0.2304\n",
            "Epoch 30/50\n",
            "14/14 [==============================] - 56s 4s/step - loss: 2.4698 - accuracy: 0.2518\n",
            "Epoch 31/50\n",
            "14/14 [==============================] - 55s 4s/step - loss: 2.4039 - accuracy: 0.2375\n",
            "Epoch 32/50\n",
            "14/14 [==============================] - 55s 4s/step - loss: 2.3648 - accuracy: 0.2708\n",
            "Epoch 33/50\n",
            "14/14 [==============================] - 56s 4s/step - loss: 2.3490 - accuracy: 0.2565\n",
            "Epoch 34/50\n",
            "14/14 [==============================] - 54s 4s/step - loss: 2.2032 - accuracy: 0.3373\n",
            "Epoch 35/50\n",
            "14/14 [==============================] - 53s 4s/step - loss: 2.2732 - accuracy: 0.2874\n",
            "Epoch 36/50\n",
            "14/14 [==============================] - 53s 4s/step - loss: 2.3076 - accuracy: 0.2755\n",
            "Epoch 37/50\n",
            "14/14 [==============================] - 54s 4s/step - loss: 2.2046 - accuracy: 0.3017\n",
            "Epoch 38/50\n",
            "14/14 [==============================] - 53s 4s/step - loss: 2.2296 - accuracy: 0.2945\n",
            "Epoch 39/50\n",
            "14/14 [==============================] - 53s 4s/step - loss: 2.1814 - accuracy: 0.3040\n",
            "Epoch 40/50\n",
            "14/14 [==============================] - 53s 4s/step - loss: 2.1222 - accuracy: 0.3587\n",
            "Epoch 41/50\n",
            "14/14 [==============================] - 54s 4s/step - loss: 2.2123 - accuracy: 0.2898\n",
            "Epoch 42/50\n",
            "14/14 [==============================] - 52s 4s/step - loss: 2.1677 - accuracy: 0.3183\n",
            "Epoch 43/50\n",
            "14/14 [==============================] - 53s 4s/step - loss: 2.0650 - accuracy: 0.3183\n",
            "Epoch 44/50\n",
            "14/14 [==============================] - 53s 4s/step - loss: 2.1223 - accuracy: 0.2993\n",
            "Epoch 45/50\n",
            "14/14 [==============================] - 54s 4s/step - loss: 2.1417 - accuracy: 0.3112\n",
            "Epoch 46/50\n",
            "14/14 [==============================] - 54s 4s/step - loss: 2.0741 - accuracy: 0.3278\n",
            "Epoch 47/50\n",
            "14/14 [==============================] - 53s 4s/step - loss: 2.0769 - accuracy: 0.3515\n",
            "Epoch 48/50\n",
            "14/14 [==============================] - 53s 4s/step - loss: 2.0647 - accuracy: 0.3563\n",
            "Epoch 49/50\n",
            "14/14 [==============================] - 53s 4s/step - loss: 2.0808 - accuracy: 0.3563\n",
            "Epoch 50/50\n",
            "14/14 [==============================] - 53s 4s/step - loss: 2.1290 - accuracy: 0.3254\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the number of classes (food categories) in your dataset\n",
        "num_classes = 22  # Replace 10 with the actual number of food categories in your dataset\n",
        "\n",
        "# Define the image dimensions and batch size\n",
        "img_width, img_height = 224, 224\n",
        "batch_size = 32\n",
        "\n",
        "# Define the path to your training data directory\n",
        "train_data_dir = '/content/drive/MyDrive/Food Recognization/Split_data/training_set'\n",
        "\n",
        "# Create ImageDataGenerator for training data with aggressive data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Load and augment training data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Define your CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))  # Add dropout with a rate of 0.5\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(lr=0.0001)  # Experiment with different learning rates\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "epochs = 50  # Increase the number of epochs for training\n",
        "model.fit(train_generator, epochs=epochs)\n",
        "\n",
        "# Save the trained model for future use\n",
        "model.save('food_classification_model.h5')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM48+XZBlRB7iiyKGFSt3Xq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}